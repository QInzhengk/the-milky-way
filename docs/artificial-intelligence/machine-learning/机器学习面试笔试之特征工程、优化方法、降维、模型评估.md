@[TOC](机器学习面试笔试之特征工程、优化方法、降维、模型评估)
# [微信公众号：数学建模与人工智能](https://mp.weixin.qq.com/s?__biz=MzI5MTY1MzU1Mg==&mid=2247487933&idx=1&sn=7bf999a20800e41806cb05b65098bb4f&chksm=ec0c0362db7b8a74a266afc842b45e7b2b53f2e8eb4b6609af105e2cf0b4aae13d03c6e0dc1e&token=1104317395&lang=zh_CN#rd)
# 一、特征工程有哪些？
特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数据的过程。在实际工作中，**特征工程旨在去除原始数据中的杂质和冗余**，设计更高效的特征以刻画求解的问题与预测模型之间的关系。
主要讨论以下两种常用的数据类型。

 - 结构化数据。结构化数据类型可以看作关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型；每一行数据表示一个样本的信息。
 - 非结构化数据。非结构化数据主要包括文本、图像、音频、视频数据， 其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数据的大小各不相同。
## 1.特征归一化
为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得不同指标之间具有可比性。例如，分析一个人的身高和体重对健康的影响，如果使用米（m）和千克（kg）作为单位，那么身高特征会在1.6～1.8m的数值范围内，体重特征会在50～100kg的范围内，分析出来的结果显然会倾向于数值差别比较大的体重特征。想要得到更为准确的结果，就需要进行特征归一化 （Normalization）处理，使各指标处于同一数值量级，以便进行分析。
对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内。最常用的方法主要有以下两种。
 - **线性函数归一化（Min-Max Scaling）**。它对原始数据进行线性变换，使结果映射到[0, 1]的范围，实现对原始数据的等比缩放。归一化公式如下，其中X为原始数据，

![](/255ac3be2a2647fcb675992050e226f6.png)

 分别为数据最大值和最小值。

![添加图片注释，不超过 140 字（可选）](/42874f50c1f54aa9873b9d6d8a37cf4d.png)

 - **零均值归一化**（Z-Score Normalization）。它会将原始数据映射到均值为 0、标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为σ，那么归一化公式定义为


优点：**训练数据归一化后，容易更快地通过梯度下降找到最优解。**

![添加图片注释，不超过 140 字（可选）](/59ab6f1960ad4a79ba05e0af0ad5e24f.png)

当然，数据归一化并不是万能的。在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。**但对于决策树模型则并不适用。**
## 2.类别型特征
类别型特征（Categorical Feature）主要是指性别（男、女）、血型（A、B、 AB、O）等只在有限选项内取值的特征。类别型特征原始输入通常是字符串形式，**除了决策树等少数模型能直接处理字符串形式的输入**，对于逻辑回归、支持向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。
### 序号编码
**序号编码通常用于处理类别间具有大小关系的数据**。例如成绩，可以分为低、中、高三档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对类别型特征赋予一个数值ID，例如高表示为3、中表示为2、低表示为1，转换后依然保留了大小关系。
### 独热编码(one-hot)
独热编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有4个取值（A型血、B型血、AB型血、O型血），独热编码会把血型变成一个4维稀疏向量，A型血表示为（1, 0, 0, 0），B型血表示为（0, 1, 0, 0），AB型表示为（0, 0, 1, 0），O型血表示为（0, 0, 0, 1）。对于类别取值较多的情况下使用独热编码。
### 二进制编码
二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。以A、B、AB、O血型为例，下图是二进制编码的过程。A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为 010；以此类推可以得到AB型血和O型血的二进制表示。

![添加图片注释，不超过 140 字（可选）](/2d0a79df55c94fca9bc93706d8d6db47.png)

## 3.高维组合特征的处理
为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。以广告点击预估问题为例，原始数据有语言和类型两种离散特征，第一张图是语言和类型对点击的影响。为了提高拟合能力，语言和类型可以组成二阶特征，第二张图是语言和类型的组合特征对点击的影响。


## 4.文本表示模型
文本是一类非常重要的非结构化数据，如何表示文本数据一直是机器学习领域的一个重要研究方向。
### 词袋模型和N-gram模型
最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开， 然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重。
### 主题模型
主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。
### 词嵌入与深度学习模型
词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。
## 5.其它特征工程

 1. 如果某个特征当中有**缺失值**，缺失比较少的话，可以使用该特征的平均值或者其它比较靠谱的数据进行填充；缺失比较多的话可以考虑删除该特征。
 2. 可以分析特征与结果的相关性，把相关性小的特征去掉。
 3. 当用户使用稀疏特征进行训练时，对于离散特征缺省值应该如何处理效果较好（**对缺省值赋给一个全新值来标记**）
## 6.特征工程脑图

![添加图片注释，不超过 140 字（可选）](/1bb559a9b7d74c58913fef69997e7aa5.png)

# 二、机器学习优化方法（优化算法）

 - **损失函数**：是定义在单个样本上的，算的是一个样本的误差
 - **代价函数**：是定义在整个训练集上的，是所有样本误差的平均，即损失函数的平均
 - **目标函数**：最终要优化的函数，等于经验风险+结构风险，对于目标函数来说，再有约束条件下的最小化就是损失函数

优化是应用数学的一个分支，也是机器学习的核心组成部分。实际上，机器学习算法 = 模型表征 + 模型评估 + 优化算法。其中，优化算法所做的事情就是在模型表征空间中找到模型评估指标最好的模型。不同的优化算法对应的模型表征和评估指标不尽相同。
**机器学习算法的关键一环是模型评估， 而损失函数定义了模型的评估指标。**
## 1.机器学习常用损失函数
损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。常见的损失函数如下：
### 平方损失函数

Y-f(X)表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和。而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：


该损失函数一般使用在线性回归当中。
### log损失函数

![添加图片注释，不超过 140 字（可选）](/0a9c320419c543c3a568a3af59126e04.png)

该损失函数一般使用在逻辑回归中。
### Hinge损失函数

![添加图片注释，不超过 140 字（可选）](/1cffcd3254a842d9aec5d65b3a98b04c.png)

SVM采用的就是Hinge Loss，用于“最大间隔(max-margin)”分类。
## 2.什么是凸优化（对于凸优化问题，所有的局部极小值都是全局最小值）
**凸函数**的严格定义为，函数L(·) 是凸函数当且仅当对定义域中的任意两点x，y和任意实数λ∈[0,1]总有：


该不等式的一个直观解释是，凸函数曲面上任意两点连接而成的线段，其上的任意一点都不会处于该函数曲面的下方，如下图所示。


凸优化问题的例子包括支持向量机、线性回归等线性模型，非凸优化问题的例子包括低秩模型（如矩阵分解）、深度神经网络模型等。
主成分分析对应的优化问题是非凸优化问题，但可以借助SVD（奇异值分解）直接得到主成分分析的全局极小值。
## 3.正则化项
为什么希望模型参数具有稀疏性呢？稀疏性，说白了就是模型的很多参数是0 。这相当于对模型进行了一次特征选择，只留下一些比较重要的特征，提高模型的泛化能力，降低过拟合的可能。
从概率角度出发，**L1正则和L2正则分别是假设参数服从laplace分布/高斯分布**。
## 4.常见的几种最优化方法
### 梯度下降法
梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的搜索迭代示意图如下图所示：

![添加图片注释，不超过 140 字（可选）](/d9aa8814ff6c4df3b29b642d4a55571d.png)

**缺点**：靠近极小值时收敛速度减慢；直线搜索时可能会产生一些问题；可能会“之字形”地下降。
### 牛顿法
牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f(x)的泰勒级数的前面几项来寻找方程f(x)=0的根。牛顿法最大的特点就在于它的收敛速度很快。具体步骤：

 - 首先，选择一个接近函数 f(x)零点的 x0，计算相应的f(x0)和切线斜率f'(x0)（这里f'表示函数f的导数）。
 - 然后我们计算穿过点(x0, f(x0))并且斜率为f'(x0)的直线和x轴的交点的x坐标，也就是求如下方程的解：
 - 我们将新求得的点的x坐标命名为x1，通常x1会比x0更接近方程f(x)=0的解。因此我们现在可以利用x1开始下一轮迭代。

由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法搜索动态示例图：

从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快**。缺点**：

 - 牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
 - 在高维情况下这个矩阵非常大，计算和存储都是问题。
 - 在小批量的情况下，牛顿法对于二阶导数的估计噪声太大。
 - 目标函数非凸的时候，牛顿法容易受到鞍点或者最大值点的吸引。
### 拟牛顿法
拟牛顿法是求解非线性优化问题最有效的方法之一，**本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。**拟牛顿法和梯度下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于梯度下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。
### 共轭梯度法
共轭梯度法是介于梯度下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。
下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：


**当训练数据量特别大肘，经典的梯度下降法存在什么问题，需要做如何改进？**
经典的梯度下降法在每次对模型参数进行更新时，需要遍历所有的训练数据。当M很大时，这需要很大的计算量，耗费很长的计算时间，在实际应用中基本不可行。
**随机梯度下降并没有引入非线性**
**AdaGrad 使用的是一阶导数；L-BFGS 使用的是二阶导数**
为了解决该问题，随机梯度下降法（ Stochastic Gradient Descent,SGD ）用单个训练样本的损失来近似平均损失。随机梯度下降法用单个训练数据即可对模型参数进行一次更新，大大加快了收敛速率。该方法也非常适用于数据源源不断到来的在线更新场景。
为了降低随机梯度的方差，从而使得迭代算法更加稳定，也为了充分利用高度优化的矩阵运算操作，在实际应用中我们会同时处理若干训练数据， 该方法被称为小批量梯度下降法（ Mini-Batch Gradient Descent ）

 - Mini-batch比随机梯度下降噪声更小
 - Mini-batch的梯度下降单次iteration速度比批梯度下降快
 - 不同的Mini-Batch训练出来的BN参数是不相同的
 - Batch Gradient Descent（BGD）对所有函数不可收敛到全局极小值
# 三、降维方法
常见的降维方法有主成分分析、线性判别分析、等距映射、局部线性嵌入、拉普拉斯特征映射、局部保留投影、MDS多维缩放、流行学习。
## 1.线性判别分析（LDA）
线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。
LDA分类思想简单总结如下：
多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。
对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。
对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。
如果用一句话概括LDA思想，即**“投影后类内方差最小，类间方差最大”**。
假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。


左图和右图是两种不同的投影方式。
​

 - 左图思路：让不同类别的平均点距离最远的投影方式。
 - ​ 右图思路：让同类别的数据挨得最近的投影方式。

​ 从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。
​ 以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。
**优缺点**

![添加图片注释，不超过 140 字（可选）](/1224b12b9fea42e7ba932f1112bd17a7.png)

## 2.主成分分析（PCA）(目标：最大化投影方差)

 1. PCA就是将高维的数据通过线性变换投影到低维空间上去。
 2. 投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。
 3. 去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。
 4. 去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。
 5. 对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。
 6. 完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。
 7. 之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。
### 图解PCA
PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n < m），这n维形成主元，是重构出来最能代表原始数据的正交特征。
假设数据集是m个n维，  。如果n=2，需要降维到n'=1，现在想找到某一维度方向代表这两个维度的数据。下图有  ,  两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？

![添加图片注释，不超过 140 字（可选）](/3980943ec8454bc4ad236d481f4a71fc.png)

从图可看出，  比  好，为什么呢？有以下两个主要评价指标：

 1. 样本点到这个直线的距离足够近。
 2. 样本点在这个直线上的投影能尽可能的分开。

如果我们需要降维的目标维数是其他任意维，则：

 1. 样本点到这个超平面的距离足够近。
 2. 样本点在这个超平面上的投影能尽可能的分开。

可以通过核映射对PCA 进行扩展得到核主成分分析（ KPCA ） ， 也可以通过流形映射的障维方法，比如等距映射、局部结性嵌入、拉普拉斯特征映射等，对一些PCA 效果不好的复杂数据集进行非线性降维操作。
**优缺点（各主成分之间正交）**


## 3.比较这两种方法
### 降维的必要性：

 1. 多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。
 2. 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。
 3. 过多的变量，对查找规律造成冗余麻烦。
 4. 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。
### 降维的目的：
 5. 减少预测变量的个数。
 6. 确保这些变量是相互独立的。
 7. 提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。
 8. 数据在低维下更容易处理、更容易使用。
 9. 去除数据噪声。
 10. 降低算法运算开销。
### LDA和PCA区别


# 四、机器学习评估方法
混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。具体评价指标有总体精度、制图精度、用户精度等，这些精度指标从不同的侧面反映了图像分类的精度。下图为## 混淆矩阵

![添加图片注释，不超过 140 字（可选）](/e719b1ea9b584908967d6e46a092b01c.png)

## 1.准确率(Accuracy)
准确率（Accuracy）。顾名思义，就是所有的预测正确（正类负类）的占总的比重。


准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。所以，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。
## 2.精确率（Precision）
精确率（Precision），查准率。是指分类正确的正样本个数占分类器判定为正样本的样本个数的比例。

![添加图片注释，不超过 140 字（可选）](/df5164bda0864419ab7d6b48b58089e2.png)

## 3.召回率(Recall)
召回率（Recall），查全率。是指分类正确的正样本个数占真正的正样本个数的比例。


### 精准率与召回率相互制约、相互平衡
例题：假设二元分类的输出是概率值，一般设定输出概率大于或等于 0.5，则预测为正类；若输出概率小于 0.5，则预测为负类。那么，**如果将阈值 0.5 提高，则准确率（Precision）增加或者不变和召回率（Recall）减小或者不变，反之。**
为了综合评估一个排序模型的好坏，不仅要看模型在不同 Top N下的Precision@N和Recall@N，而且最好绘制出模型的P-R（Precision- Recall）曲线。这里简单介绍一下P-R曲线的绘制方法。
**P-R曲线的横轴是召回率，纵轴是精确率**。对于一个排序模型来说，其P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本， 小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R 曲线是通过将阈值从高到低移动而生成的。下图是P-R曲线样例图，其中实线代表模型A的P-R曲线，虚线代表模型B的P-R曲线。原点附近代表当阈值最大时模型的精确率和召回率。

![添加图片注释，不超过 140 字（可选）](/c083eca2b6d74631916f54dc9f1c4550.png)

由图可见，当召回率接近于0时，模型A的精确率为0.9，模型B的精确率是1， 这说明模型B得分前几位的样本全部是真正的正样本，而模型A即使得分最高的几个样本也存在预测错误的情况。并且，随着召回率的增加，精确率整体呈下降趋势。但是，当召回率为1时，模型A的精确率反而超过了模型B。**这充分说明，只用某个点对应的精确率和召回率是不能全面地衡量模型的性能，只有通过P-R曲线的整体表现，才能够对模型进行更为全面的评估。**
## 4.F1值(H-mean值)
F1值（H-mean值）。越大越好，是精准率和召回率的调和平均值。

![添加图片注释，不超过 140 字（可选）](/3241ddd5873041088956153be2acc063.png)


![添加图片注释，不超过 140 字（可选）](/be12c9201a46466f810b375af986b521.png)

## 5.ROC曲线
ROC曲线。接收者操作特征曲线（receiver operating characteristic curve），是反映敏感性和特异性连续变量的综合指标，ROC曲线上每个点反映着对同一信号刺激的感受性。下图是ROC曲线例子。

![添加图片注释，不超过 140 字（可选）](/33daaaeb505a4af1a66f9b0dc594f676.png)

横坐标：1-Specificity，伪正类率(False positive rate，FPR，FPR=FP/(FP+TN))，预测为正但实际为负的样本占所有负例样本的比例；
纵坐标：Sensitivity，真正类率(True positive rate，TPR，TPR=TP/(TP+FN))，预测为正且实际为正的样本占所有正例样本的比例。

**真正的理想情况，TPR应接近1，FPR接近0，即图中的（0,1）点。ROC曲线越靠拢（0,1）点，越偏离45度对角线越好。**

**ROC曲线与P-R曲线有什么特点？**
相比P-R曲线， ROC曲线有一个特点，当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化。
这个特点让ROC曲线能够尽量降低不同测试集带来的干扰，更加客观地衡量模型本身的性能。这有什么实际意义呢？在很多实际问题中， 正负样本数量往往很不均衡。比如，计算广告领域经常涉及转化率模型，正样本的数量往往是负样本数量的1/1000 甚至1/10000 。若选择不同的测试集，P-R曲线的变化就会非常大，而ROC曲线则能够更加稳定地反映模型本身的好坏。所以，ROC曲线的适用场景更多，被广泛用于排序、推荐、广告等领域。但需要注意的是，选择P-R曲线还是ROC曲线是因实际问题而异的，如果研究者希望更多地看到模型在特定数据集上的表现，P-R 曲线则能够更直观地反映其性能。
### AUC值
AUC (Area Under Curve) 被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。
从AUC判断分类器（预测模型）优劣的标准：

 - AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
 - 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
 - AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
 - AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

一句话来说，AUC值越大的分类器，正确率越高。
## 6.余弦距离和欧式距离
### 余弦距离： 
余弦距离是一种衡量两个非零向量之间角度差异的方法，常用于文本分析、推荐系统等领域。其值范围在0到1之间，值越小表示两个向量越相似。
### 欧式距离
在数学中，欧几里得距离或欧几里得度量是欧几里得空间中两点间“普通”（即直线）距离。
对于两个向量A和B，余弦距离关注的是向量之间的角度关系，并不关心它们的绝对大小，其取值范围是[−1,1]。当一对文本相似度的长度差距很大、但内容相近时，如果使用词频或词向量作为特征，它们在特征空间中的的欧氏距离通常很大；而如果使用余弦相似度的话，它们之间的夹角可能很小，因而相似度高。此外，在文本、图像、 视频等领域，研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保持“相同时为1，正交时为0，相反时为−1”的性质，而欧氏距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。
在机器学习领域，被俗称为距离，却不满足三条距离公理的不仅仅有余弦距离，还有KL 距离（ Kullback-Leibler Divergence ），也叫作相对熵，它常用于计算两个分布之间的差异，但不满足对称性和三角不等式。
## 7.A/B测试
AB测试是为Web或App界面或流程制作两个（A/B）或多个（A/B/n）版本，在同一时间维度，分别让组成成分相同（相似）的访客群组（目标人群）随机的访问这些版本，收集各群组的用户体验数据和业务数据，最后分析、评估出最好版本，正式采用。
## 8.模型评估方法(划分训练集和测试集的方法)
### Holdout检验（留出法）
Holdout 检验是最简单也是最直接的验证方法，它将原始的样本集合随机划分成训练集和验证集两部分。比方说，对于一个点击率预测模型，我们把样本按照 70%～30% 的比例分成两部分，70% 的样本用于模型训练；30% 的样本用于模型验证，包括绘制ROC曲线、计算精确率和召回率等指标来评估模型性能。
Holdout 检验的缺点很明显，即在验证集上计算出来的最后评估指标与原始分组有很大关系。为了消除随机性，研究者们引入了“交叉检验”的思想。
### 交叉检验（增大k会导致交叉验证结果的置信度增加）
k-fold交叉验证：首先将全部样本划分成k个大小相等的样本子集；依次遍历这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把k次评估指标的平均值作为最终的评估指标。在实际实验中，k经常取10。
k=n：留一验证，每次留下1个样本作为验证集，其余所有样本作为测试集。样本总数为n ，依次对n个样本进行遍历，进行n次验证，再将评估指标求平均值得到最终的评估指标。
### 自助法
不管是Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的。然而，当样本规模比较小时，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。有没有能维持训练集样本规模的验证方法呢？自助法可以比较好地解决这个问题。
自助法是基于自助采样法的检验方法。对于总数为n的样本集合，进行**n次有放回**的随机抽样，得到大小为n的训练集。n次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。
## 9.超参数调优
为了进行超参数调优，我们一般会采用网格搜索、随机搜索、贝叶斯优化等算法。在具体介绍算法之前，需要明确超参数搜索算法一般包括哪几个要素。一是目标函数，即算法需要最大化/最小化的目标；二是搜索范围，一般通过上限和下限来确定；三是算法的其他参数，如搜索步长。

 - **网格搜索**，可能是最简单、应用最广泛的超参数搜索算法，它通过查找搜索范围内的所有的点来确定最优值。如果采用较大的搜索范围以及较小的步长，网格搜索有很大概率找到全局最优值。然而，**这种搜索方案十分消耗计算资源和时间**，特别是需要调优的超参数比较多的时候。因此，在实际应用中，网格搜索法一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。这种操作方案可以降低所需的时间和计算量，但由于目标函数一般是非凸的，所以很可能会错过全局最优值。
 - **随机搜索**，随机搜索的思想与网格搜索比较相似，只是不再测试上界和下界之间的所有值，而是在搜索范围中随机选取样本点。它的理论依据是，如果样本点集足够大，那么通过随机采样也能大概率地找到全局最优值，或其近似值。随机搜索一般会比网格搜索要快一些，但是和网格搜索的快速版一样，它的结果也是没法保证的。
 - **贝叶斯优化算法**，贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息； 而贝叶斯优化算法则充分利用了之前的信息。贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。
## 10.过拟合和欠拟合（偏差和方差）
**泛化能力**（generalization ability）是指机器学习算法对新鲜样本的适应能力。
### 偏差(Bias)
反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精度
### 方差(Variance)
反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性


**模型误差=偏差(Bias)+方差(Variance)+不可避免的误差**
过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。欠拟合指的是模型在训练和预测时表现都不好的情况。下图形象地描述了过拟合和欠拟合的区别。

![添加图片注释，不超过 140 字（可选）](/71ed8bfc3021466a8ff16c41a2dbb588.png)

### 防止过拟合（过拟合是高方差）：

 - 从数据入手，获得更多的训练数据。（使用更多的训练数据是解决过拟合问题最高效的手段，因为更多的样本能够让模型学习到更多更高效的特征，减小噪声的影响）
 - 增加样本数
 - 降低模型复杂度。
 - 正则化方法，给模型的参数加上一定的正则约束。
 - 集成学习方法，集成学习是把多个模型集成在一起。
### 防止欠拟合（欠拟合是高偏差）：
 - 添加新特征。
 - 增加模型复杂度。
 - 减小正则化系数。
 - 偏差和方差通常都是矛盾的。
 - 降低偏差，会提高方差。
 - 降低方差，会提高偏差。

机器学习算法的主要挑战，来自于方差！​

**例题**：存在两种文本分类算法——A和B，对于同一组样本数据，A和B识别出了同样数量的正类数据，但A比B识别出了更多的正确的正类数据，则针对两种算法的查准率P和查全率R，则（P(A)>P(B)、R(A)>R(B) ）。

参考：
​


@[TOC](机器学习面试笔试知识点-线性回归、逻辑回归Logistics Regression和支持向量机SVM)
# [微信公众号：数学建模与人工智能](https://mp.weixin.qq.com/s?__biz=MzI5MTY1MzU1Mg==&mid=2247487933&idx=1&sn=7bf999a20800e41806cb05b65098bb4f&chksm=ec0c0362db7b8a74a266afc842b45e7b2b53f2e8eb4b6609af105e2cf0b4aae13d03c6e0dc1e&token=1104317395&lang=zh_CN#rd)
# 一、线性回归
## 1.线性回归的假设函数
![在这里插入图片描述](/8d4d216a9ab04161b878c521bc880b0b.png)

## 2.线性回归的损失函数（Loss Function）

**MSE（均方误差）** 

![](/075c517239ef428da3c26511129f923d.png)

 
通过梯度下降法或正规方程
![在这里插入图片描述](/323af78291e64b9b8981fa8dcd1bf4be.png)
求出使得代价函数最小的参数

### 两者区别
![](/b484e812ec7944bcb0368dc7fe641664.png)

## 3.简述岭回归与Lasso回归以及使用场景
**目的**![在这里插入图片描述](/ffc9068ddcd34b20bc9e0455fd5f4740.png)

**本质**：约束（限制）要优化的参数

这两种回归均通过在损失函数中引入正则化项来达到目的：
![在这里插入图片描述](/89d728a157c74588b8b5e35932bc8013.png)

**L1正则化容易得到稀疏矩阵**
## 4.什么场景下用L1、L2正则化
 L2正则化会使参数的绝对值变小，增强模型的稳定性（不会因为数据变化而产生很大的震荡）；而L1正则化会使一些参数为0，可以实现特征稀疏，增强模型解释性。
## 5.什么是ElasticNet回归
**综合了L1正则化项和L2正则化项：**

![添加图片注释，不超过 140 字（可选）](/71001899bc834cfcb8a699ef8c5ae06f.png)
## 6.ElasticNet回归的使用场景
使用Lasso回归太过(太多特征被稀疏为0),而岭回归正则化的不够(回归系数衰减太慢)的时候，可以考虑ElasticNet回归。

**L1正则化和L2正则化分别是假设参数服从laplace分布和高斯分布。**

**线性回归中的残差服从均值为0的正态分布。**

**L2:权重衰减**
## 7.线性回归要求因变量服从正态分布？(持保留态度)
假设线性回归的噪声服从均值为0的正态分布。 当噪声符合正态分布N(0,  )时，因变量则符合正态分布N(ax(i)+b,  )，其中预测函数y=ax(i)+b。这个结论可以由正态分布的概率密度函数得到。也就是说当噪声符合正态分布时，其因变量必然也符合正态分布。 
在用线性回归模型拟合数据之前，首先要求数据应符合或近似符合正态分布，否则得到的拟合函数不正确。
# 二、逻辑回归(Logistics Regression)
## 1.本质：极大似然估计
逻辑回归是用来做分类算法的。把线性回归的结果Y代入一个非线性变换的**Sigmoid函数**中，即可得到[0,1]之间取值范围的数S，S可以把它看成是一个概率值，如果设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类。
## 2.激活函数：Sigmoid

![添加图片注释，不超过 140 字（可选）](/c06f4e0222a74d49aa58fdd1a1ba0354.png)

## 3.损失函数：对数损失函数(log loss)

![添加图片注释，不超过 140 字（可选）](/c89c7d90fdcb47c59a58b7a8f831ab8d.png)

公式中的 y=1 表示的是真实值为1时用第一个公式， y=0 表示真实值为0时用第二个公式计算损失。

**为什么要加上log函数呢？**
    当真实样本为1时，但h=0，那么log0=∞，即对模型最大的惩罚力度；当h=1时，那么log1=0，相当于没有惩罚，也就是没有损失，达到最优结果。把上面损失函数写成统一的形式：

![添加图片注释，不超过 140 字（可选）](/5055feb5cbf04aee95c94e7995f479e8.png)

## 4.代价函数：交叉熵(Cross Entropy)：

![添加图片注释，不超过 140 字（可选）](/632ce66f0edd43e9be2b355467392780.png)

最后按照梯度下降法，求解极小值点，得到想要的模型效果。
## 5.可以进行多分类吗？
可以 ，从二分类问题过度到多分类问题(one vs rest)，思路步骤如下：

 1. 将类型class1看作正样本，其他类型全部看作负样本，可得到样本标记类型为该类型的概率p1。
 2. 然后再将另外类型class2看作正样本，其他类型全部看作负样本，同理得到p2。
 3. 以此循环，我们可以得到该待预测样本的标记类型分别为类型class i时的概率pi，最后取pi中最大的那个概率对应的样本标记类型作为待预测样本类型。                    

![添加图片注释，不超过 140 字（可选）](/d1ae4f579abb4bbc8b0de4a03782799d.png)

总之还是以二分类来依次划分，并求出最大概率结果。
## 6.逻辑回归优缺点
### 优

 - 能以概率的形式输出结果，而非只是0,1判定。
 - 可解释性强，可控度高，训练快。
 - 因为结果是概率，可以做ranking model（排序模型）。
### 缺
 - 对模型中自变量多重共线性较为敏感。

## 7.逻辑回归有哪些应用

 1. CTR预估/推荐系统的learning to rank/各种分类场景。
 2. 某搜索引擎厂的广告CTR（点击率）预估基线版是LR。
 3. 某电商搜索排序/广告CTR预估基线版是LR。
 4. 某电商的购物搭配推荐用了大量LR。
 5. 某现在一天广告赚1000w+的新闻app排序基线是LR。
## 8.逻辑回归为什么要对特征进行离散化。
 6. **非线性**！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；
 7. **速度快**！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 
 8. **鲁棒性**！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0\
 9. 如果特征没有离散化，一个异常数据“年龄200岁”会给模型造成很大的干扰； 
方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 
简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。


**若要求多分类，需要把sigmoid换成softmax**

实战中，设置  

**损失函数、代价函数、目标函数**
损失函数（Loss Function ）**是定义在单个样本上的，算的是一个样本的误差。**
代价函数（Cost Function ）**是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。**
目标函数（Object Function）定义为：**最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。对于目标函数来说在有约束条件下的最小化就是损失函数。**
# 机器学习中的最优化方法
## 1.梯度下降法
优化思想：用当前位置负梯度方向作为搜索方向。
## 2.牛顿法
使用函数f(x)的泰勒级数的前几项来寻找f(x)的根
优：以本质上看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法更快。
缺：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hesssian矩阵的逆矩阵，计算较复杂。
## 3.拟牛顿法
本质思想：改善牛顿法内次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian（黑塞）矩阵的逆，从而简化运算的复杂度。
## 4.共轭梯度法
仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵的逆的特点。
# 三、支持向量机(SVM)
一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。
## 函数间隔
给定一个超平面(w,b)，定义该超平面关于样本点  的函数间隔为：  ，定义该超平面关于训练集T的函数间隔为：

![添加图片注释，不超过 140 字（可选）](/08e522816ff44982b70761e6f53936e5.png)

## 几何间隔（geometrical margin）

![添加图片注释，不超过 140 字（可选）](/2a34f964cc694a5d870d9e55e7bfe104.png)

## 1.损失函数：合页损失函数（Hinge loss）
SVM的损失函数就是合页损失函数+正则项
 ![在这里插入图片描述](/d8d62d54e6604f26b2dcbd433da84835.png)

## 2.为什么要将求解SVM的原始问题转换为其对偶问题？

 1. 对偶问题更容易求解，
 2. 可以自然引入核函数，进而推广到非线性分类问题。

**（若原问题与对偶问题均存在可行解，则两者均存在最优解）。**
## 3.支持向量
距离超平面最近的且满足一定条件的几个样本点。
## 4.带核的SVM为什么能分类非线性问题
核函数的本质是两个函数的内积，通过核函数将其映射到高维空间，在高维空间非线性问题转换为线性问题，SVM得到超平面是高维空间的线性分类平面，其分类结果也视为低维空间的非线性分类结果。
## 5.SVM的应用
SVM在很多诸如文本分类，图像分类，生物序列分析和生物数据挖掘，手写字符识别等领域有很多的应用。
## 6. 如何选择核函数？

 - 如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；
 - 如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；
 - 如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。
## 7.LR和SVM的联系与区别
**相同点**
 - 如果不考虑核函数，都是线性分类器。
 - 都是监督学习算法。
 - 都是判别模型。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。

**不同点**

 - （本质区别）：目标函数不同，逻辑回归是log loss，SVM采用的是hinge loss（铰链损失函数），这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
 - LR是参数模型，svm是非参数模型。
 - 与SVM相比，LR对异常数据更加敏感。
 - SVM的目标是结构风险最小化，逻辑回归目标函数是最小化先验概率。
 - 在训练集较小时，SVM较适用（基于距离分类），需要对数据先做归一化；LR则需要更多的样本（基于概率分类）
 - SVM只考虑支持向量（support vectors），也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 
## 8.加入松弛变量的SVM的训练误差可以为0吗？
使用SMO（序列最小优化算法）算法训练的线性分类器并不一定能得到训练误差为0的模型。这是由于优化目标改变了，并不再是使训练误差最小。
 

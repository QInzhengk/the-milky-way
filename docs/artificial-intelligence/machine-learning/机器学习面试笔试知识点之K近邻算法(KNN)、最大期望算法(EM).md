@[TOC](机器学习面试笔试知识点之K近邻算法KNN、最大期望算法EM)
# [微信公众号：数学建模与人工智能](https://mp.weixin.qq.com/s?__biz=MzI5MTY1MzU1Mg==&mid=2247487933&idx=1&sn=7bf999a20800e41806cb05b65098bb4f&chksm=ec0c0362db7b8a74a266afc842b45e7b2b53f2e8eb4b6609af105e2cf0b4aae13d03c6e0dc1e&token=1104317395&lang=zh_CN#rd)
# 一、K近邻算法(KNN)（监督学习算法）
## 1. 什么是KNN
### 1.1 KNN的通俗解释
何谓K近邻算法，即K-Nearest Neighbor algorithm，简称KNN算法，单从名字来猜想，可以简单粗暴的认为是：K个最近的邻居，当K=1时，算法便成了最近邻算法，即寻找最近的那个邻居。
用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，**在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。**

![添加图片注释，不超过 140 字（可选）](/a7b1a4c57aa14d2fad03627c5acb3049.png)


![添加图片注释，不超过 140 字（可选）](/38d601bee3db441d8c18ff864da95caf.png)

如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），KNN就是解决这个问题的。
如果K=3，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。
如果K=5，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。
**于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。**
### 1.2 近邻的距离度量
我们看到，K近邻算法的核心在于找到实例点的邻居，这个时候，问题就接踵而至了，如何找到邻居，邻居的判定标准是什么，用什么来度量。这一系列问题便是下面要讲的距离度量表示法。

**有哪些距离度量的表示法：**

**欧氏距离（旋转不变性）**
最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点  和  之间的距离为：

![添加图片注释，不超过 140 字（可选）](/9d23c9f4b8c3473990a7fdccfea1d153.png)

二维平面上两点  与  间的欧氏距离：

![添加图片注释，不超过 140 字（可选）](/030ce31bfe454bc0bbe5e72c7dc96a37.png)

也可以用表示成向量运算的形式：

![添加图片注释，不超过 140 字（可选）](/1772786a812b423a9e0f67319972e409.png)

**曼哈顿距离**
我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。
通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，此即曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。
二维平面上两点  与  间的曼哈顿距离

两个n维向量  与  间的曼哈顿距离

![添加图片注释，不超过 140 字（可选）](/0bd0135362204c6ebd82e6f3ed869f72.png)

**切比雪夫距离**
若二个向量或二个点p 、q，其座标分别为  及  ，则两者之间的切比雪夫距离定义如下：

![添加图片注释，不超过 140 字（可选）](/ad86dbdca0ca48b9a0ce134b3ce5ca57.png)

这也等于以下Lp度量的极值： 


，因此切比雪夫距离也称为L∞度量。
以数学的观点来看，切比雪夫距离是由一致范数（uniform norm）（或称为上确界范数）所衍生的度量，也是超凸度量（injective metric space）的一种。
在平面几何中，若二点p及q的直角坐标系坐标为  及  ，则切比雪夫距离为：

![添加图片注释，不超过 140 字（可选）](/1919d27ba6b348838ff33686693480da.png)

**玩过国际象棋的朋友或许知道，国王走一步能够移动到相邻的8个方格中的任意一个。那么国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？你会发现最少步数总是max( | x2-x1 | , | y2-y1 | ) 步 。有一种类似的一种距离度量方法叫切比雪夫距离。**
两个n维向量 与 间的切比雪夫距离：

这个公式的另一种等价形式是

**闵可夫斯基距离(Minkowski Distance)**
闵氏距离不是一种距离，而是一组距离的定义。
两个n维变量 与 间的闵可夫斯基距离定义为：

![添加图片注释，不超过 140 字（可选）](/c0a3b4a2455e4646b59f6d926334543b.png)

其中p是一个变参数。 当p=1时，就是曼哈顿距离 当p=2时，就是欧氏距离 当p→∞时，就是切比雪夫距离。根据变参数的不同，闵氏距离可以表示一类的距离。

**标准化欧氏距离**
标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。至于均值和方差标准化到多少，先复习点统计学知识。
假设样本集X的数学期望或均值(mean)为m，标准差(standard deviation，方差开根)为s，那么X的“标准化变量”X*表示为：(X-m）/s，而且标准化变量的数学期望为0，方差为1。 即，样本集的标准化过程(standardization)用公式描述就是：

![添加图片注释，不超过 140 字（可选）](/8fb9abcca51a42e7938b8bcab0f5525d.png)

标准化后的值 = ( 标准化前的值 － 分量的均值 ) /分量的标准差，经过简单的推导就可以得到两个n维向量 与 间的标准化欧氏距离的公式：


**马氏距离**
有M个样本向量  ，协方差矩阵记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为：

![添加图片注释，不超过 140 字（可选）](/70d3d34ccd6a4cd09e49368386a5b3c1.png)

若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了,也就是欧氏距离了：

![添加图片注释，不超过 140 字（可选）](/3540933507a64320bf85792f1cc3415b.png)

若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。
马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。

**巴氏距离**
在统计中，巴氏距离距离测量两个离散或连续概率分布的相似性。它与衡量两个统计样品或种群之间的重叠量的巴氏距离系数密切相关。Bhattacharyya系数可以被用来确定两个样本被认为相对接近的，它是用来测量中的类分类的可分离性。
对于离散概率分布 p和q在同一域 X，它被定义为：

![添加图片注释，不超过 140 字（可选）](/445ed6a851ff451f8f603d0ee2bcf122.png)

其中：

![添加图片注释，不超过 140 字（可选）](/15abc4020bbe404ca9a6731188e078a9.png)

是Bhattacharyya系数。

**汉明距离**
两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。

**夹角余弦**
几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。
在二维空间中向量  与向量  的夹角余弦公式：

![添加图片注释，不超过 140 字（可选）](/696f410f1a2645bb8bf11b0ccbf46da1.png)

两个n维样本点 与 的夹角余弦：


夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。

**杰卡德相似系数**
两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。杰卡德相似系数是衡量两个集合的相似度一种指标。


与杰卡德相似系数相反的概念是杰卡德距离：


**皮尔逊系数**
在统计学中，皮尔逊积矩相关系数用于度量两个变量X和Y之间的相关（线性相关），其值介于-1与1之间。通常情况下通过以下取值范围判断变量的相关强度：
0.8-1.0 极强相关 0.6-0.8 强相关 0.4-0.6 中等程度相关 0.2-0.4 弱相关 0.0-0.2 极弱相关或无相关

简单说来，各种“距离”的应用场景简单概括为，

 1. 空间：欧氏距离，
 2. 路径：曼哈顿距离，国际象棋国王：切比雪夫距离，
 3. 以上三种的统一形式:闵可夫斯基距离，
 4. 加权：标准化欧氏距离，
 5. 排除量纲和依存：马氏距离，
 6. 向量差距：夹角余弦，
 7. 编码差别：汉明距离，
 8. 集合近似度：杰卡德类似系数与距离，
 9. 相关：相关系数与相关距离。
### 1.3 K值选择
 1. 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，**K值的减小就意味着整体模型变得复杂，容易发生过拟合；**
 2. 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且**K值的增大就意味着整体的模型变得简单。**
 3. K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。
在实际应用中，K值一般取一个比较小的数值，**例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。**
### 1.4 KNN最近邻分类算法的过程
 13. 计算测试样本和训练样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 
 14. 对上面所有的距离值进行排序；选前 k 个最小距离的样本；
 15. 根据这 k 个样本的标签进行投票，得到最后的分类类别；
## 2.KDD的实现：KD树
Kd-树是K-dimension tree的缩写，是对数据点在k维空间（如二维(x，y)，三维(x，y，z)，k维(x，y，z..)）中划分的一种数据结构，主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。本质上说，Kd-树就是一种平衡二叉树。
首先必须搞清楚的是，k-d树是一种空间划分树，说白了，就是把整个空间划分为特定的几个部分，然后在特定空间的部分内进行相关搜索操作。想像一个三维(多维有点为难你的想象力了)空间，kd树按照一定的划分规则把这个三维空间划分了多个空间，如下图所示：


### 2.1 构建KD树
kd树构建的伪代码如下图所示：

![添加图片注释，不超过 140 字（可选）](/15bf2a991695476592c1d37c460be488.png)

再举一个简单直观的实例来介绍k-d树构建算法。假设有6个二维数据点{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，数据点位于二维空间内，如下图所示。为了能有效的找到最近邻，k-d树采用分而治之的思想，即将整个空间划分为几个小部分，首先，粗黑线将空间一分为二，然后在两个子空间中，细黑直线又将整个空间划分为四部分，最后虚黑直线将这四部分进一步划分。

![添加图片注释，不超过 140 字（可选）](/af1357f220284930a044116a9e36cf04.png)

6个二维数据点{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}构建kd树的具体步骤为：

 1. 确定：split域=x。具体是：6个数据点在x，y维度上的数据方差分别为39，28.63，所以在x轴上方差更大，故split域值为x；
 2. 确定：Node-data = （7,2）。具体是：根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为7，所以Node-data域位数据点（7,2）。这样，该节点的分割超平面就是通过（7,2）并垂直于：split=x轴的直线x=7；
 3. 确定：左子空间和右子空间。具体是：分割超平面x=7将整个空间分为两部分：x<=7的部分为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点={(9,6)，(8,1)}；
 4. 如上算法所述，kd树的构建是一个递归过程，我们对左子空间和右子空间内的数据重复根节点的过程就可以得到一级子节点（5,4）和（9,6），同时将空间和数据集进一步细分，如此往复直到空间中只包含一个数据点。
与此同时，经过对上面所示的空间划分之后，我们可以看出，点(7,2)可以为根结点，从根结点出发的两条红粗斜线指向的(5,4)和(9,6)则为根结点的左右子结点，而(2,3)，(4,7)则为(5,4)的左右孩子(通过两条细红斜线相连)，最后，(8,1)为(9,6)的左孩子(通过细红斜线相连)。如此，便形成了下面这样一棵k-d树：

![添加图片注释，不超过 140 字（可选）](/d50f4a31965b4a8da212feb2a5d8f748.png)

**对于n个实例的k维数据来说，建立kd-tree的时间复杂度为O(knlogn)。**
k-d树算法可以分为两大部分，除了上部分有关k-d树本身这种数据结构建立的算法，另一部分是在建立的k-d树上各种诸如插入，删除，查找(最邻近查找)等操作涉及的算法。下面，咱们依次来看kd树的插入、删除、查找操作。
### 2.2 KD树的插入
元素插入到一个K-D树的方法和二叉检索树类似。本质上，在偶数层比较x坐标值，而在奇数层比较y坐标值。当我们到达了树的底部，（也就是当一个空指针出现），我们也就找到了结点将要插入的位置。生成的K-D树的形状依赖于结点插入时的顺序。给定N个点，其中一个结点插入和检索的平均代价是O(log2N)。
插入的过程如下:

![添加图片注释，不超过 140 字（可选）](/a66a2c8496c147dabd9e9cff0442cb1e.png)

应该清楚，这里描述的插入过程中，每个结点将其所在的平面分割成两部分。因比，Chicago 将平面上所有结点分成两部分，一部分所有的结点x坐标值小于35，另一部分结点的x坐标值大于或等于35。同样Mobile将所有x坐标值大于35的结点以分成两部分，一部分结点的Y坐标值是小于10，另一部分结点的Y坐标值大于或等于10。后面的Toronto、Buffalo也按照一分为二的规则继续划分。
### 2.3 KD树的删除
KD树的删除可以用递归程序来实现。我们假设希望从K-D树中删除结点（a,b）。如果（a,b）的两个子树都为空，则用空树来代替（a,b）。否则，在（a,b）的子树中寻找一个合适的结点来代替它，譬如(c,d)，则递归地从K-D树中删除（c,d）。一旦(c,d)已经被删除，则用（c,d）代替（a,b）。假设(a,b)是一个X识别器，那么，它得替代节点要么是（a,b）左子树中的X坐标最大值的结点，要么是（a,b）右子树中x坐标最小值的结点。
下面来举一个实际的例子(来源：中国地质大学电子课件，原课件错误已经在下文中订正)，如下图所示，原始图像及对应的kd树，现在要删除图中的A结点，请看一系列删除步骤：

![添加图片注释，不超过 140 字（可选）](/b11b2f59be4043ef919e9a81374e2757.png)

要删除上图中结点A，选择结点A的右子树中X坐标值最小的结点，这里是C，C成为根，如下图：

![添加图片注释，不超过 140 字（可选）](/ef7f30f4282a435b982c0dd404184968.png)

从C的右子树中找出一个结点代替先前C的位置，

![添加图片注释，不超过 140 字（可选）](/82a6caea8dd24a5aba8acda9c2501ea2.png)

这里是D，并将D的左子树转为它的右子树，D代替先前C的位置，如下图：

![添加图片注释，不超过 140 字（可选）](/d01a6d9ab7284e10a6387efb62894382.png)

在D的新右子树中，找X坐标最小的结点，这里为H，H代替D的位置，

![添加图片注释，不超过 140 字（可选）](/37b1905048a64dbcb566da6c2beb86ac.png)

在D的右子树中找到一个Y坐标最小的值，这里是I，将I代替原先H的位置，从而A结点从图中顺利删除，如下图所示：

![添加图片注释，不超过 140 字（可选）](/6ce72144a9b54e2f951fbaa49992887c.png)

从K-D树中删除一个结点是代价很高的，很清楚删除子树的根受到子树中结点个数的限制。用TPL（T）表示树T总的路径长度。可看出树中子树大小的总和为TPL（T）+N。 以随机方式插入N个点形成树的TPL是O(N*log2N),这就意味着从一个随机形成的K-D树中删除一个随机选取的结点平均代价的上界是O(log2N) 。
### 2.4 KD树的最近邻搜索算法
k-d树查询算法的伪代码如下所示：


递归版本的二维kd tree的搜索函数对比：

![添加图片注释，不超过 140 字（可选）](/00bf20e5081e4cce850e6c61cc1901e0.png)

**举例**
星号表示要查询的点查询点（2，4.5）。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行相关的‘回溯'操作。也就是说，算法首先沿搜索路径反向查找是否有距离查询点更近的数据点。

 1. **二叉树搜索**：先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径<(7,2)，(5,4)，(4,7)>，但（4,7）与目标查找点的距离为3.202，而（5,4）与查找点之间的距离为3.041，所以（5,4）为查询点的最近点；
 2. **回溯查找**：以（2，4.5）为圆心，以3.041为半径作圆，如下图所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找，也就是将（2,3）节点加入搜索路径中得<(7,2)，(2,3)>；于是接着搜索至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5；
 3. 回溯查找至（5,4），直到最后回溯到根结点（7,2）的时候，以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如下图所示。至此，搜索路径回溯完，返回最近邻点（2,3），最近距离1.5。

![添加图片注释，不超过 140 字（可选）](/bbb743f9d4b64e1783c44955a7e6fdc4.png)


![添加图片注释，不超过 140 字（可选）](/2cc29d0f50b7437f954f13a7dfec03a2.png)


### 2.5 kd树近邻搜索算法的改进：BBF算法
实例点是随机分布的，那么kd树搜索的平均计算复杂度是O（logN），这里的N是训练实例树。所以说，kd树更适用于训练实例数远大于空间维数时的k近邻搜索，当空间维数接近训练实例数时，**它的效率会迅速下降，一降降到“解放前”：线性扫描的速度。**
也正因为上述k最近邻搜索算法的第4个步骤中的所述：“回退到根结点时，搜索结束”，每个最近邻点的查询比较完成过程最终都要回退到根结点而结束，而导致了许多不必要回溯访问和比较到的结点，这些多余的损耗在高维度数据查找的时候，搜索效率将变得相当之地下，那有什么办法可以改进这个原始的kd树最近邻搜索算法呢？
从上述标准的kd树查询过程可以看出其搜索过程中的“回溯”是由“查询路径”决定的，并没有考虑查询路径上一些数据点本身的一些性质。一个简单的改进思路就是将“查询路径”上的结点进行排序，如按各自分割超平面（也称bin）与查询点的距离排序，**也就是说，回溯检查总是从优先级最高（Best Bin）的树结点开始。**
还是以上面的查询（2,4.5）为例，搜索的算法流程为：

 1. 将（7,2）压人优先队列中；
 2. 提取优先队列中的（7,2），由于（2,4.5）位于（7,2）分割超平面的左侧，所以检索其左子结点（5,4）。
 3. 同时，根据BBF机制”搜索左/右子树，就把对应这一层的兄弟结点即右/左结点存进队列”，将其（5,4）对应的兄弟结点即右子结点（9,6）压人优先队列中
 4. 此时优先队列为{（9,6）}，最佳点为（7,2）；然后一直检索到叶子结点（4,7），此时优先队列为{（2,3），（9,6）}，“最佳点”则为（5,4）；
 5. 提取优先级最高的结点（2,3），重复步骤2，直到优先队列为空。

![添加图片注释，不超过 140 字（可选）](/b423dfbcb6284e678a4bd0131d8edb65.png)

## 3.关于KNN的一些问题
### 1.在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？
答：我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，欧式距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。例如：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向的运动。
### 2.KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?
答：极大的节约了时间成本．点线距离如果 >最小点，无需回溯上一层，如果<,则再上一层寻找。
# 二、最大期望算法(EM) 
## 1.什么是EM算法
最大期望算法（Expectation-maximization algorithm，又译为期望最大化算法），是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量。
最大期望算法经过两个步骤交替进行计算，
**第一步**是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值； **第二步**是最大化（M），最大化在E步上求得的最大似然值来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。
极大似然估计用一句话概括就是：知道结果，反推条件θ。
## 2.似然函数
在数理统计学中，**似然函数**是一种关于统计模型中的参数的函数，表示模型参数中的似然性。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性。**而极大似然就相当于最大可能的意思。**
比如你一位同学和一位猎人一起外出打猎，一只野兔从前方窜过。只听一声枪响，野兔应声到下，如果要你推测，这一发命中的子弹是谁打的？你就会想，只发一枪便打中，由于猎人命中的概率一般大于你那位同学命中的概率，从而推断出这一枪应该是猎人射中的。
这个例子所作的推断就体现了最大似然法的基本思想。
**多数情况下我们是根据已知条件来推算结果，而最大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。**
## 3.极大似然函数的求解步骤
假定我们要从10万个人当中抽取100个人来做身高统计，那么抽到这100个人的概率就是(概率连乘)：

![添加图片注释，不超过 140 字（可选）](/409523d12b104123bc437d465b52e3d9.png)

现在要求的就是这个  值，也就是使得  的概率最大化，那么这时的参数  就是所求。
为了便于分析，我们可以定义对数似然函数，将其变成连加的形式：

![添加图片注释，不超过 140 字（可选）](/cadd7d80bb224c7a830917ade8046c17.png)

对于求一个函数的极值，通过我们在本科所学的微积分知识，最直接的设想是求导，然后让导数为0，那么解这个方程得到的θ就是了（当然，前提是函数L(θ)连续可微）。但，如果θ是包含多个参数的向量那怎么处理呢？当然是求L(θ)对所有参数的偏导数，也就是梯度了，从而n个未知的参数，就有n个方程，方程组的解就是似然函数的极值点了，最终得到这n个参数的值。
求极大似然函数估计值的一般步骤：
写出似然函数；

 1. 对似然函数取对数，并整理；
 2. 求导数，令导数为0，得到似然方程；
 3. 解似然方程，得到的参数即为所求；
## 4.EM算法
两枚硬币A和B，假定随机抛掷后正面朝上概率分别为PA，PB。为了估计这两个硬币朝上的概率，咱们轮流抛硬币A和B，每一轮都连续抛5次，总共5轮：
![在这里插入图片描述](/69e0fc15557f4e83a9ab88a6228851f1.png)

硬币A被抛了15次，在第一轮、第三轮、第五轮分别出现了3次正、1次正、2次正，所以很容易估计出PA，类似的，PB也很容易计算出来(**真实值**)，如下：
PA = （3+1+2）/ 15 = 0.4       PB= （2+3）/10 = 0.5
问题来了，如果我们不知道抛的硬币是A还是B呢（即硬币种类是隐变量），把上表硬币结果换成Unknown。
OK，问题变得有意思了。现在我们的目标没变，还是估计PA和PB，需要怎么做呢？
显然，此时我们多了一个硬币种类的隐变量，设为z，可以把它认为是一个5维的向量（z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币，比如z1，就代表第一轮投掷时使用的硬币是A还是B。

 - 但是，这个变量z不知道，就无法去估计PA和PB，所以，我们必须先估计出z，然后才能进一步估计PA和PB。
 - 可要估计z，我们又得知道PA和PB，这样我们才能用极大似然概率法则去估计z，这不是鸡生蛋和蛋生鸡的问题吗，如何破？

答案就是先随机初始化一个PA和PB，用它来估计z，然后基于z，还是按照最大似然概率法则去估计新的PA和PB，然后依次循环，如果新估计出来的PA和PB和我们真实值差别很大，**直到PA和PB收敛到真实值为止。**
我们不妨这样，先随便给PA和PB赋一个值，比如： 硬币A正面朝上的概率PA = 0.2 硬币B正面朝上的概率PB = 0.7
然后，我们看看第一轮抛掷最可能是哪个硬币。 如果是硬币A，得出3正2反的概率为 0.2*0.2*0.2*0.8*0.8 = 0.00512 如果是硬币B，得出3正2反的概率为0.03087 然后依次求出其他4轮中的相应概率。做成表格如下：

![添加图片注释，不超过 140 字（可选）](/8802feae8dc449059746b6c4a5f5cd62.png)

按照最大似然法则： 第1轮中最有可能的是硬币B 第2轮中最有可能的是硬币A 第3轮中最有可能的是硬币A 第4轮中最有可能的是硬币B 第5轮中最有可能的是硬币A
我们就把概率更大，即更可能是A的，即第2轮、第3轮、第5轮出现正的次数2、1、2相加，除以A被抛的总次数15（A抛了三轮，每轮5次），作为z的估计值，B的计算方法类似。然后我们便可以按照最大似然概率法则来估计新的PA和PB。
PA = （2+1+2）/15 = 0.33                        PB =（3+3）/10 = 0.6
就这样，不断迭代 不断接近真实值，这就是EM算法的奇妙之处。
可以期待，我们继续按照上面的思路，用估计出的PA和PB再来估计z，再用z来估计新的PA和PB，反复迭代下去，就可以最终得到PA = 0.4，PB=0.5，此时无论怎样迭代，PA和PB的值都会保持0.4和0.5不变，于是乎，我们就找到了PA和PB的最大似然估计。
总结一下计算步骤：
1.随机初始化分布参数θ
2.E步，求Q函数，对于每一个i，计算根据上一次迭代的模型参数来计算出隐性变量的后验概率（其实就是隐性变量的期望），来作为隐藏变量的现估计值：

![添加图片注释，不超过 140 字（可选）](/e148a17f6c2046eba4729fe7297175a0.png)

3.M步，求使Q函数获得极大时的参数取值）将似然函数最大化以获得新的参数值

![添加图片注释，不超过 140 字（可选）](/a195efa0c89643418409215d946f13be.png)

4.然后循环重复2、3步直到收敛。
## 5. 采用 EM 算法求解的模型有哪些？
**用EM算法求解的模型一般有GMM或者协同过滤，k-means其实也属于EM**。EM算法一定会收敛，但是可能收敛到局部最优。由于求和的项数将随着隐变量的数目指数上升，会给梯度计算带来麻烦。

参考：


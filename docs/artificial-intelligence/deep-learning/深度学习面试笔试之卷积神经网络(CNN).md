@[TOC](深度学习面试笔试之卷积神经网络CNN)
# [微信公众号：数学建模与人工智能](https://mp.weixin.qq.com/s?__biz=MzI5MTY1MzU1Mg==&mid=2247487933&idx=1&sn=7bf999a20800e41806cb05b65098bb4f&chksm=ec0c0362db7b8a74a266afc842b45e7b2b53f2e8eb4b6609af105e2cf0b4aae13d03c6e0dc1e&token=1104317395&lang=zh_CN#rd)
# 1. 什么是CNN
卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一。
我们先来看卷积神经网络各个层级结构图：

![添加图片注释，不超过 140 字（可选）](/52ba5783793c4e5a9484ddbe19ffbf7f.png)

上图中CNN要做的事情是：给定一张图片，是车还是马未知，是什么车也未知，现在需要模型判断这张图片里具体是一个什么东西，总之输出一个结果：如果是车 那是什么车。

 - 最左边是数据输入层(input layer)，对数据做一些处理，比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）、归一化（把所有的数据都归一到同样的范围）、PCA/白化等等。CNN只对训练集做“去均值”这一步。
 - CONV：卷积计算层(conv layer)，线性乘积求和。
 -  RELU：激活层(activation layer)，下文有提到：ReLU是激活函数的一种。
 - POOL：池化层(pooling layer)，简言之，即取区域平均或最大。
 - FC：全连接层(FC layer)。
这几个部分中，卷积计算层是CNN的核心。
## 1.1 输入层
在做输入的时候，需要把图片处理成同样大小的图片才能够进行处理。
常见的处理数据的方式有：
### 去均值(常用)
 - **AlexNet**：训练集中100万张图片，对每个像素点求均值，得到均值图像，当训练时用原图减去均值图像。
 - **VGG**：对所有输入在三个颜色通道R/G/B上取均值，只会得到3个值，当训练时减去对应的颜色通道均值。(此种方法效率高)

**TIPS**:在训练集和测试集上减去训练集的均值。
### 归一化
幅度归一化到同样的范围。
### PCA/白化(很少用)

 - 用PCA降维
 - 白化是对数据每个特征轴上的幅度归一化。
## 1.2 卷积计算层(conv)
对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做**内积**（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。
滤波器filter是什么呢！请看下图。图中左边部分是原始输入数据，图中中间部分是滤波器filter，图中右边是输出的新的二维数据。
不同的滤波器filter会得到不同的输出数据，比如颜色深浅、轮廓。**相当于提取图像的不同特征，模型就能够学习到多种特征**。用不同的滤波器filter，提取想要的关于图像的特定信息：颜色深浅或轮廓。如下图所示。

![添加图片注释，不超过 140 字（可选）](/0a31013417e3418b8a29c27e123c92ca.png)

在CNN中，滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据。这个过程中，有这么几个参数：

 - 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。
 - 步长stride：决定滑动多少步可以到边缘。
 - 填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。

![添加图片注释，不超过 140 字（可选）](/aa148eb024d8410e9c63b005d81002a2.png)


![添加图片注释，不超过 140 字（可选）](/4d87697ee2fb4b62855f7b5283064ee3.png)

### 参数共享机制
假设每个神经元连接数据窗的权重是固定对的。固定每个神经元连接权重，可以看做模板，每个神经元只关注**一个特性(模板)**，这使得需要估算的权重个数减少：一层中从1亿到3.5万。

 - 一组固定的权重和不同窗口内数据做**内积**：卷积
 - 作用在于捕捉某一种模式，具体表现为很大的值。

**卷积操作的本质特性包括稀疏交互和参数共享。**
## 1.3 激活层
把卷积层输出结果做非线性映射。
激活函数有：

![添加图片注释，不超过 140 字（可选）](/dc9aa2eb1ef94c5193aec105665ea96f.png)

 - sigmoid：在两端斜率接近于0，梯度消失。
 - ReLu：修正线性单元，有可能出现斜率为0，但概率很小，因为mini-batch是一批样本损失求导之和。
### TIPS:
 - CNN慎用sigmoid！慎用sigmoid！慎用sigmoid！ 
 - 首先试RELU，因为快，但要小心点。 
 - 如果RELU失效，请用Leaky ReLU或者Maxout。 
 - 某些情况下tanh倒是有不错的结果，但是很少。

## 1.4 池化层
也叫下采样层，就算通过了卷积层，纬度还是很高 ，需要进行池化层操作。

 - 夹在连续的卷积层中间。
 - 压缩数据和参数的量，降低维度。
 - 减小过拟合。
 - 具有特征不变性。

方式有：**Max pooling、average pooling**

![添加图片注释，不超过 140 字（可选）](/581394dbbc134e86b65e323c83a0071d.png)

### Max pooling
取出每个部分的最大值作为输出
### average pooling
每个部分进行计算得到平均值作为输出
## 1.5 全连接层
全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全连接的特性，一般全连接层的参数也是最多的。

 - 两层之间所有神经元都有权重连接
 - 通常全连接层在卷积神经网络尾部
## 1.6 层次结构小结

![添加图片注释，不超过 140 字（可选）](/9de8f4d3b0794296a684f38508ae01a0.png)

## 1.7 CNN优缺点
### 优点：

 - 共享卷积核，优化计算量。
 - 无需手动选取特征，训练好权重，即得特征。
 - 深层次的网络抽取图像信息丰富，表达效果好。
 - 保持了层级网络结构。
 - 不同层次有不同形式与功能。
### 缺点：
 - 需要调参，需要大样本量，GPU等硬件依赖。
 - 物理含义不明确。

**与BP网络对比，CNN网络具有的不同点是（权值共享、局部连接)**
卷积向下取整，池化向上取整。 计算每一层输出图像的size的公式。无论是卷积层还是pooling层，公式都是这样的：

```bash
(input_size+2*padding-kernel_size)/stride+1=output_size
```

其中，padding指对input的图像边界补充一定数量的像素，目的是为了计算位于图像边界的像素点的卷积响应；kernel_size指卷积核的大小；stride指步长，即卷积核或者pooling窗口的滑动位移。另外需要注意，上面公式建立在所有参数都为整数的假设基础上。
例题：假设你在卷积神经网络的第一层中有 5 个卷积核，每个卷积核尺寸为 7×7，具有零填充且步幅为 1。该层的输入图片的维度是 224×224×3。那么该层输出的维度为**（218 x 218 x 5)**

## 神经网络训练时是否可以将全部参数初始化为0？

在神经网络训练时，将所有参数初始化为0并不是一个推荐的做法。以下是原因和替代方案：
### 原因
**梯度消失问题**：
如果所有参数都初始化为0，那么在网络的前向传播过程中，每个神经元的输出将完全相同。这会导致在反向传播过程中，所有权重的梯度也相同，从而使得所有权重以相同的方式更新。这种现象称为“梯度消失”，会使得网络无法有效地学习。
**对称性问题**：
当所有权重都相同时，网络中的神经元在训练过程中会学习到相同的特征，无法实现多样化的特征提取，从而降低模型的表达能力。
### 替代方案
**随机初始化**：
使用小范围内的随机数初始化权重，可以打破对称性，使每个神经元在训练过程中学习不同的特征。常见的方法包括均匀分布和正态分布。
**Xavier 初始化**：
Xavier 初始化（也称为Glorot初始化）旨在保持每一层的输入和输出的方差大致相同，有助于加速收敛。
**He 初始化**：
He 初始化适用于ReLU激活函数，旨在解决ReLU激活函数导致的梯度消失问题。
